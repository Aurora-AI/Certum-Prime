# https://sdk.vercel.ai/cookbook/next/stream-text-with-chat-prompt

Copy markdown

[Stream Text with Chat Prompt](#stream-text-with-chat-prompt)
=============================================================

Chat completion can sometimes take a long time to finish, especially when the response is big. In such cases, it is useful to stream the chat completion to the client in real-time. This allows the client to display the new message as it is being generated by the model, rather than have users wait for it to finish.

http://localhost:3000

User: How is it going?

Assistant: All good, how may I help you?

Why is the sky blue?

Send Message

[Client](#client)
-----------------

Let's create a React component that imports the `useChat` hook from the `@ai-sdk/react` module. The `useChat` hook will call the `/api/chat` endpoint when the user sends a message. The endpoint will generate the assistant's response based on the conversation history and stream it to the client.

app/page.tsx

```
1

'use client';



2



3

import { useChat } from '@ai-sdk/react';



4

import { DefaultChatTransport } from 'ai';



5

import { useState } from 'react';



6



7

export default function Page() {



8

const [input, setInput] = useState('');



9



10

const { messages, sendMessage } = useChat({



11

transport: new DefaultChatTransport({



12

api: '/api/chat',



13

}),



14

});



15



16

return (



17

<div>



18

<input



19

value={input}



20

onChange={event => {



21

setInput(event.target.value);



22

}}



23

onKeyDown={async event => {



24

if (event.key === 'Enter') {



25

sendMessage({



26

parts: [{ type: 'text', text: input }],



27

});



28

}



29

}}



30

/>



31



32

{messages.map((message, index) => (



33

<div key={index}>



34

{message.parts.map(part => {



35

if (part.type === 'text') {



36

return <div key={`${message.id}-text`}>{part.text}</div>;



37

}



38

})}



39

</div>



40

))}



41

</div>



42

);



43

}
```

[Server](#server)
-----------------

Next, let's create the `/api/chat` endpoint that generates the assistant's response based on the conversation history.

app/api/chat/route.ts

```
1

import { convertToModelMessages, streamText, type UIMessage } from 'ai';



2



3

export async function POST(req: Request) {



4

const { messages }: { messages: UIMessage[] } = await req.json();



5



6

const result = streamText({



7

model: 'openai/gpt-4o',



8

system: 'You are a helpful assistant.',



9

messages: await convertToModelMessages(messages),



10

});



11



12

return result.toUIMessageStreamResponse();



13

}
```

---

[View Example on GitHub](https://github.com/vercel/ai/blob/main/examples/next-openai-pages/pages/chat/stream-chat/index.tsx)
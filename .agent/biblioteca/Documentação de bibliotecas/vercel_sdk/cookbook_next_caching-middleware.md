# https://sdk.vercel.ai/cookbook/next/caching-middleware

Copy markdown

[Caching Middleware](#caching-middleware)
=========================================

This example is not yet updated to v5.

Let's create a simple chat interface that uses [`LanguageModelMiddleware`](/docs/ai-sdk-core/middleware) to cache the assistant's responses in fast KV storage.

[Client](#client)
-----------------

Let's create a simple chat interface that allows users to send messages to the assistant and receive responses. You will integrate the `useChat` hook from `@ai-sdk/react` to stream responses.

app/page.tsx

```
1

'use client';



2



3

import { useChat } from '@ai-sdk/react';



4



5

export default function Chat() {



6

const { messages, input, handleInputChange, handleSubmit, error } = useChat();



7

if (error) return <div>{error.message}</div>;



8



9

return (



10

<div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">



11

<div className="space-y-4">



12

{messages.map(m => (



13

<div key={m.id} className="whitespace-pre-wrap">



14

<div>



15

<div className="font-bold">{m.role}</div>



16

{m.toolInvocations ? (



17

<pre>{JSON.stringify(m.toolInvocations, null, 2)}</pre>



18

) : (



19

<p>{m.content}</p>



20

)}



21

</div>



22

</div>



23

))}



24

</div>



25



26

<form onSubmit={handleSubmit}>



27

<input



28

className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"



29

value={input}



30

placeholder="Say something..."



31

onChange={handleInputChange}



32

/>



33

</form>



34

</div>



35

);



36

}
```

[Middleware](#middleware)
-------------------------

Next, you will create a `LanguageModelMiddleware` that caches the assistant's responses in KV storage.
`LanguageModelMiddleware` has two methods: `wrapGenerate` and `wrapStream`.
`wrapGenerate` is called when using [`generateText`](/docs/reference/ai-sdk-core/generate-text) and [`generateObject`](/docs/reference/ai-sdk-core/generate-object), while `wrapStream` is called when using [`streamText`](/docs/reference/ai-sdk-core/stream-text) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object).

For `wrapGenerate`, you can cache the response directly.
Instead, for `wrapStream`, you cache an array of the stream parts, which can then be used with [`simulateReadableStream`](/docs/reference/ai-sdk-core/simulate-readable-stream) function to create a simulated `ReadableStream` that returns the cached response.
In this way, the cached response is returned chunk-by-chunk as if it were being generated by the model.
You can control the initial delay and delay between chunks by adjusting the `initialDelayInMs` and `chunkDelayInMs` parameters of `simulateReadableStream`.

ai/middleware.ts

```
1

import { Redis } from '@upstash/redis';



2

import {



3

type LanguageModelV1,



4

type LanguageModelV3Middleware,



5

type LanguageModelV1StreamPart,



6

simulateReadableStream,



7

} from 'ai';



8



9

const redis = new Redis({



10

url: process.env.KV_URL,



11

token: process.env.KV_TOKEN,



12

});



13



14

export const cacheMiddleware: LanguageModelV3Middleware = {



15

wrapGenerate: async ({ doGenerate, params }) => {



16

const cacheKey = JSON.stringify(params);



17



18

const cached = (await redis.get(cacheKey)) as Awaited<



19

ReturnType<LanguageModelV1['doGenerate']>



20

> | null;



21



22

if (cached !== null) {



23

return {



24

...cached,



25

response: {



26

...cached.response,



27

timestamp: cached?.response?.timestamp



28

? new Date(cached?.response?.timestamp)



29

: undefined,



30

},



31

};



32

}



33



34

const result = await doGenerate();



35



36

redis.set(cacheKey, result);



37



38

return result;



39

},



40

wrapStream: async ({ doStream, params }) => {



41

const cacheKey = JSON.stringify(params);



42



43

// Check if the result is in the cache



44

const cached = await redis.get(cacheKey);



45



46

// If cached, return a simulated ReadableStream that yields the cached result



47

if (cached !== null) {



48

// Format the timestamps in the cached response



49

const formattedChunks = (cached as LanguageModelV1StreamPart[]).map(p => {



50

if (p.type === 'response-metadata' && p.timestamp) {



51

return { ...p, timestamp: new Date(p.timestamp) };



52

} else return p;



53

});



54

return {



55

stream: simulateReadableStream({



56

initialDelayInMs: 0,



57

chunkDelayInMs: 10,



58

chunks: formattedChunks,



59

}),



60

};



61

}



62



63

// If not cached, proceed with streaming



64

const { stream, ...rest } = await doStream();



65



66

const fullResponse: LanguageModelV1StreamPart[] = [];



67



68

const transformStream = new TransformStream<



69

LanguageModelV1StreamPart,



70

LanguageModelV1StreamPart



71

>({



72

transform(chunk, controller) {



73

fullResponse.push(chunk);



74

controller.enqueue(chunk);



75

},



76

flush() {



77

// Store the full response in the cache after streaming is complete



78

redis.set(cacheKey, fullResponse);



79

},



80

});



81



82

return {



83

stream: stream.pipeThrough(transformStream),



84

...rest,



85

};



86

},



87

};
```

This example uses `@upstash/redis` to store and retrieve the assistant's
responses but you can use any KV storage provider you would like.

[Server](#server)
-----------------

Finally, you will create an API route for `api/chat` to handle the assistant's messages and responses. You can use your cache middleware by wrapping the model with `wrapLanguageModel` and passing the middleware as an argument.

app/api/chat/route.ts

```
1

import { cacheMiddleware } from '@/ai/middleware';



2

import { wrapLanguageModel, streamText, tool } from 'ai';



3

import { z } from 'zod';



4



5

const wrappedModel = wrapLanguageModel({



6

model: 'openai/gpt-4o-mini',



7

middleware: cacheMiddleware,



8

});



9



10

export async function POST(req: Request) {



11

const { messages } = await req.json();



12



13

const result = streamText({



14

model: wrappedModel,



15

messages,



16

tools: {



17

weather: tool({



18

description: 'Get the weather in a location',



19

inputSchema: z.object({



20

location: z.string().describe('The location to get the weather for'),



21

}),



22

execute: async ({ location }) => ({



23

location,



24

temperature: 72 + Math.floor(Math.random() * 21) - 10,



25

}),



26

}),



27

},



28

});



29

return result.toUIMessageStreamResponse();



30

}
```